{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to perform all popular regression algorithms automatically (part of ubml (my custom python package to automate ML): https://ubml.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_calc(metric_name, actual_data_train, pred_data_train, actual_data_test, pred_data_test):\n",
    "    \"\"\"\n",
    "    This function calculates the metric score\n",
    "\n",
    "    Inputs: \n",
    "        - metric_name (str): e.g r2_score from sklearn.metrics \n",
    "\n",
    "        - actual_data_train (output from sklearn train_test_split()): x_train\n",
    "        - pred_data_train (output or model.predict(x_train)): y_pred_train\n",
    "            These will be used to compute metric score on training set\n",
    "        - actual_data_test (output from sklearn train_test_split()): x_test\n",
    "        - pred_data_test (output or model.predict(x_test)): y_pred_test \n",
    "            These will be used to compute metric score on testing set\n",
    "    Return:\n",
    "        - computed metric score on traing and testing data (list): [result_train, result_test] \n",
    "    \"\"\"\n",
    "    # importing importlib to dynamically import the metrics from sklearn\n",
    "    import importlib        \n",
    "    # creating special rule for RMSE as its not available directly in sklearn.metrics\n",
    "    if metric_name == \"root_mean_squared_error\":\n",
    "        # import mean_squared_error from sklearn.metrics\n",
    "        metric = getattr(importlib.import_module(\"sklearn.metrics\"), \"mean_squared_error\")\n",
    "        # calculating RMSE on training and testing predictions by doing sqaured=False on MSE\n",
    "        result_train = metric(actual_data_train, pred_data_train, squared=False)\n",
    "        result_test = metric(actual_data_test, pred_data_test, squared=False)\n",
    "    else:\n",
    "        # importing different metrics (it will run in a loop , every iteration will change the metric name)\n",
    "        metric = getattr(importlib.import_module(\"sklearn.metrics\"), metric_name)\n",
    "        result_train = metric(actual_data_train, pred_data_train)\n",
    "        result_test = metric(actual_data_test, pred_data_test)\n",
    "    return [result_train, result_test]\n",
    "\n",
    "def regression_train_test(x_train, y_train, x_test, y_test, metric=\"r2_score\", export_best=None, export_model=None, path=None):\n",
    "    \"\"\"\n",
    "    This function will perform regression\n",
    "\n",
    "    Inputs: \n",
    "        - x_train (output from sklearn train_test_split()): x_train\n",
    "        - y_train (output from sklearn train_test_split()): y_train\n",
    "        - x_test (output from sklearn train_test_split()): x_test\n",
    "        - y_test (output from sklearn train_test_split()): y_test\n",
    "        - metric (str): Give it a metric name for which you want to choose best model\n",
    "            By default it will take value as r2_score and choose the best model with the highest validation r2_score, but you can change the metric\n",
    "        - export_best (bool): set it as True if you want to export the best model as a pickle file \n",
    "        - export_model (str): give it a model name if you want to export that specific model's pickle file\n",
    "            models can be selected as follows:\n",
    "                Linear Regression\n",
    "                Lasso Regression\n",
    "                Ridge Regression\n",
    "                Support Vector Regression\n",
    "                Decision Tree Regression\n",
    "                Random Forest Regression\n",
    "        - path (str): give a path with model name if you want to specify where to save the model and by what name\n",
    "    Returns:\n",
    "        - Metrics Performance Table (type: A Pandas Dataframe)\n",
    "        - Best Model Name\n",
    "        - Best Model pickle file (only if you set argument export_best=True)\n",
    "        - Custom Model pickle file (only if you set argument export_model=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    # importing regression models\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    # initializing models \n",
    "    linear_reg = LinearRegression()\n",
    "    lasso_reg = Lasso()\n",
    "    ridge_reg = Ridge()\n",
    "    svr_reg = SVR(kernel = 'rbf')\n",
    "    decision_tree_reg = DecisionTreeRegressor()\n",
    "    random_forest_reg = RandomForestRegressor()\n",
    "\n",
    "    # training models\n",
    "    models = [linear_reg, lasso_reg, ridge_reg, svr_reg, decision_tree_reg, random_forest_reg]\n",
    "    for i in models:\n",
    "        i.fit(x_train, y_train)\n",
    "\n",
    "    # doing predictions on training data\n",
    "    train_pred_res = []\n",
    "    # in first iteration of loop, linear_reg will be used for prediction, in second iteration lasso_reg will be used and son\n",
    "    for i in models:\n",
    "        # appending predictions on each model in a list (train_pred_res), Now train_pred_res[0] will predictions of linear regression and train_pred_res[1] will be of lasso and so on \n",
    "        train_pred_res.append(i.predict(x_train))\n",
    "\n",
    "    # doing predictions on testing data\n",
    "    test_pred_res = []\n",
    "    for i in models:\n",
    "        test_pred_res.append(i.predict(x_test))\n",
    "\n",
    "    # computing metrics\n",
    "    regression_models_names = [\"Linear Regression\", \"Lasso Regression\", \"Ridge Regression\", \"Support Vector Regression\", \"Decision Tree Regression\", \"Random Forest Regression\"]\n",
    "    metrics_names = [\"r2_score\", \"mean_squared_error\", \"root_mean_squared_error\", \"mean_absolute_error\", \"explained_variance_score\", \"max_error\"]\n",
    "\n",
    "    model_pred_counter = 0\n",
    "    all_models_metrics = []\n",
    "    for i in range(len(regression_models_names)):\n",
    "        metrics_result = []\n",
    "        for i in range(len(metrics_names)):\n",
    "            # in first iteration, i will be 0 so metrics_names[0] will be r2_score and model_pred_counter will also be 0 so train_pred_res[0] and test_pred_res[0] will be prediction results for linear regression (check prediction step above, if don't understand: basically train_pred_res is a list and train_pred_res[0] is prediction result for linear regression and train_pred_res[1] is for lasso and so on)\n",
    "            # so the result will be output of r2_score for training and testing, it will be a list as result[0] will be r2_score for training set and result[1] will be r_2 score for testing data \n",
    "            # In second iteration i will be 1 so metrics_names[1] will be mean_squared_error so same thing will be done for that and so on for every metric included in above created list metrics_names \n",
    "            result = metric_calc(metrics_names[i], y_train, train_pred_res[model_pred_counter], y_test, test_pred_res[model_pred_counter])\n",
    "            # appending result in a list metrics_result, so now metrics_result[0] will be \n",
    "            metrics_result.extend([(format(float(result[0]), \"f\")), result[1]])\n",
    "        all_models_metrics.append(metrics_result)\n",
    "        model_pred_counter += 1\n",
    "\n",
    "    # making dictionary \n",
    "    resultant_dicts = {}   \n",
    "    for i in range(len(regression_models_names)):\n",
    "        resultant_dicts[regression_models_names[i]] = all_models_metrics[i]\n",
    "\n",
    "    row_names = [\"r2 score train\", 'r2 score test', \"mean squared error train\", \"mean squared error test\", \"root mean squared error train\", \"root mean squared error test\", \"mean absolute error train\", \"mean absolute error test\", \"explained variance score train\", \"explained variance score test\", \"max error train\", \"max error test\"]\n",
    "\n",
    "    import pandas as pd\n",
    "    resultant_df = pd.DataFrame(resultant_dicts, index=row_names)\n",
    "    resultant_df = resultant_df.transpose().astype(float).round(3)\n",
    "\n",
    "    metric_test = metric.replace(\"_\", \" \") + \" test\"\n",
    "    val_acc = []\n",
    "    for i in range(6):\n",
    "        val_acc.append(resultant_df.iloc[i][metric_test])\n",
    "    for i in range(len(val_acc)):\n",
    "        if val_acc[i] == 1:\n",
    "            val_acc[i] = -333\n",
    "    best_model = regression_models_names[val_acc.index(max(val_acc))]\n",
    "\n",
    "    if export_best:\n",
    "        import pickle\n",
    "        model_mapping = {}\n",
    "        for i in range(len(models)):\n",
    "            model_mapping[regression_models_names[i]] = models[i]\n",
    "        if path:\n",
    "            pickle.dump(model_mapping[best_model], open(path, \"wb\"))\n",
    "        else:\n",
    "            pickle.dump(model_mapping[best_model], open(\"best_model.pkl\", \"wb\")) \n",
    "\n",
    "    if export_model:\n",
    "        import pickle\n",
    "        model_mapping = {}\n",
    "        for i in range(len(models)):\n",
    "            model_mapping[regression_models_names[i]] = models[i]\n",
    "        if path:\n",
    "            pickle.dump(model_mapping[export_model], open(path, \"wb\"))\n",
    "        else:\n",
    "            pickle.dump(model_mapping[export_model], open(export_model.replace(\" \", \"_\") + \".pkl\", \"wb\"))\n",
    "        \n",
    "\n",
    "    return resultant_df, best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function to add two number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "def add(num_1, num_2):\n",
    "    return num_1+num_2\n",
    "\n",
    "print(add(5, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "def multiply(num_1, num_2):\n",
    "    \"\"\"\n",
    "    This function multiply two numbers\n",
    "    Parameters:\n",
    "        - num_1 (int) : first number to multiply\n",
    "        - num_2 (int) : second number to multiply\n",
    "    \n",
    "    Returns:\n",
    "        - resultant product of to numbers (int) \n",
    "\n",
    "    \"\"\"\n",
    "    return num_1*num_2\n",
    "\n",
    "print(multiply(5, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't Repaet Yourself (DRY Coding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Now we have written code for add and muliplication, so if we want to create a calulator now, we can simply use these already written codes \n",
    "\n",
    "def calculator(num_1, num_2):\n",
    "    print(add(num_1, num_2))\n",
    "    print(multiply(num_1, num_2))\n",
    "\n",
    "calculator(5, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *args in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def add_with_variable_num_of_args(*args):\n",
    "    print(np.array(list(args)).sum())\n",
    "\n",
    "add_with_variable_num_of_args(4, 7, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 7, 8)\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "def add_with_variable_num_of_args(*args):\n",
    "    print(args)\n",
    "    print(type(args))\n",
    "\n",
    "add_with_variable_num_of_args(4, 7, 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
